{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "from synderm.splits.train_test_splitter import synthetic_train_val_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "synthetic_derm = \"lukemelas/synthetic-derm\"  # 440 total images in each class using all_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test mutually exclusive.\n"
     ]
    }
   ],
   "source": [
    "# Load in the labels we are using for training data\n",
    "metadata_train = pd.read_csv(\"/n/data1/hms/dbmi/manrai/derm/Fitzpatrick17k/fitzpatrick17k_10label_clean_training.csv\")\n",
    "top_n_labels = metadata_train[\"label\"].value_counts().index[:9]\n",
    "metadata_train = metadata_train[metadata_train[\"label\"].isin(top_n_labels)].reset_index(drop=True)\n",
    "metadata_train['location'] = 'Fitzpatrick17k/finalfitz17k/' + metadata_train['md5hash'] + '.jpg'\n",
    "metadata_train[\"synthetic\"] = False\n",
    "\n",
    "# These are the labels we are using for testing\n",
    "metadata_test = pd.read_csv(\"/n/data1/hms/dbmi/manrai/derm/Fitzpatrick17k/fitzpatrick17k_10label_clean_held_out_set.csv\")\n",
    "metadata_test = metadata_test[metadata_test[\"label\"].isin(top_n_labels)].reset_index(drop=True)\n",
    "metadata_test[\"synthetic\"] = False\n",
    "\n",
    "ids_train = set(metadata_train[\"md5hash\"])\n",
    "ids_test = set(metadata_test[\"md5hash\"])\n",
    "\n",
    "if ids_train.isdisjoint(ids_test):\n",
    "    print(\"train/test mutually exclusive.\")\n",
    "else:\n",
    "    print(\"train/test not mutually exclusive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model with 32 real training images per disease condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model with 32 real images per class, with and without synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image directory and fastai path\n",
    "image_dir = \"/n/data1/hms/dbmi/manrai/derm/\"\n",
    "path = Path(image_dir)\n",
    "\n",
    "# Set the random seed\n",
    "random_state = 111108\n",
    "\n",
    "# Set the generation folder\n",
    "generation_folder = \"all_generations/finetune-inpaint/\"\n",
    "generation_type = \"inpaint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_synthetic_per_real = 10\n",
    "\n",
    "# First, the dataset is duplicated n_synthetic_per_real times\n",
    "df = pd.concat([metadata_train]*n_synthetic_per_real, ignore_index=True)\n",
    "\n",
    "# create a variable that represents the nth copy of the image\n",
    "df['n'] = df.groupby('md5hash').cumcount()\n",
    "df['location'] = generation_folder + df['label'].str.replace(' ', '-')  + '/' + generation_type +'/0' + df['n'].astype(str) + '/' + df['md5hash'] + '.png'\n",
    "df['synthetic'] = True\n",
    "df['Qc'] = ''\n",
    "\n",
    "# drop the 'n' column\n",
    "df = df.drop(columns=['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = synthetic_train_val_split(\n",
    "    real_data=metadata_train, \n",
    "    synthetic_data = df, \n",
    "    per_class_size=40,\n",
    "    n_real_per_class = 32,\n",
    "    random_state = random_state,\n",
    "    class_column = \"label\",\n",
    "    mapping_real_to_synthetic = \"md5hash\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'is_valid' column\n",
    "train['is_valid'] = False\n",
    "val['is_valid'] = True\n",
    "\n",
    "df = pd.concat([train, val]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust batch size based on number of images\n",
    "if (len(df[df.is_valid == False])/10 >= 100):\n",
    "    batch_size = 64\n",
    "elif (len(df[df.is_valid == False])/10 >= 10):\n",
    "    batch_size = 32\n",
    "else:\n",
    "    batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fastai dataloader\n",
    "dls = ImageDataLoaders.from_df(df, \n",
    "                        path,\n",
    "                        fn_col='location',\n",
    "                        label_col='label',\n",
    "                        valid_col='is_valid', \n",
    "                        bs=batch_size,\n",
    "                        item_tfms=Resize(224),\n",
    "                        batch_tfms=augmentation_dict[augmentation])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config parameters for wandb\n",
    "config = dict (\n",
    "    architecture = \"EfficientNet-V2-M\",\n",
    "    gen_folder = generation_folder,\n",
    "    random_state = random_state,\n",
    "    augmentation = augmentation, \n",
    "    n_training_per_label = n_per_label,\n",
    "    include_synthetic = include_synthetic,\n",
    "    n_synthetic_per_real = n_synthetic_per_real,\n",
    "    generation_type = generation_type\n",
    ")\n",
    "\n",
    "# set tags for wandb and\n",
    "sample_tag = \"n_real_per_label_\" + str(n_per_label)\n",
    "seed_tag = \"seed_\" + str(random_state)\n",
    "include_synthetic_tag = \"include_synthetic_\" + str(include_synthetic)\n",
    "generation_type_tag = str(generation_type)\n",
    "\n",
    "wandb.init(\n",
    "    project=wandb_project,\n",
    "    tags=[sample_tag, seed_tag, include_synthetic_tag, generation_type_tag],\n",
    "    config=config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with wandb callback\n",
    "learn.fit(50, cbs=[WandbCallback(), EarlyStoppingCallback (monitor='valid_loss', min_delta=0.0, patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "test_dl = dls.test_dl(test_data)\n",
    "# get predictions and probabilities for test set\n",
    "preds, _ = learn.get_preds(dl=test_dl)\n",
    "# get predicted labels for top-1 and top-3\n",
    "top1_pred = torch.argmax(preds, dim=1)\n",
    "top3_pred = torch.topk(preds, k=3, dim=1).indices\n",
    "\n",
    "md5hashes = test_data['md5hash']\n",
    "# get predicted labels and probabilities for top-1 and top-3\n",
    "top1_prob, top1_label = torch.topk(preds, k=1, dim=1)\n",
    "top3_prob, top3_label = torch.topk(preds, k=3, dim=1)\n",
    "\n",
    "# convert tensor labels to class labels\n",
    "top1_label = [learn.dls.vocab[i] for i in top1_label.squeeze()]\n",
    "top3_label = [[learn.dls.vocab[j] for j in i] for i in top3_label]\n",
    "\n",
    "# get true labels for test set\n",
    "true_labels = test_data['label']\n",
    "\n",
    "# calculate accuracy scores\n",
    "top1_acc = (top1_label == true_labels).mean()\n",
    "top3_acc = torch.zeros(len(true_labels))\n",
    "for i in range(len(true_labels)):\n",
    "    top3_acc[i] = true_labels[i] in top3_label[i]\n",
    "top3_acc = top3_acc.mean()\n",
    "\n",
    "# calculate upper and lower bounds for 95% confidence interval\n",
    "top1_ci_lower, top1_ci_upper = proportion_confint(top1_acc*len(true_labels), len(true_labels), alpha=0.05, method='normal')\n",
    "top3_ci_lower, top3_ci_upper = proportion_confint(top3_acc*len(true_labels), len(true_labels), alpha=0.05, method='normal')\n",
    "\n",
    "# log accuracy scores to wandb\n",
    "wandb.log({'top1_acc': top1_acc,\n",
    "            'top1_ci_lower': top1_ci_lower,\n",
    "            'top1_ci_upper': top1_ci_upper,\n",
    "            'top3_acc': top3_acc,\n",
    "            'top3_ci_lower': top3_ci_lower,\n",
    "            'top3_ci_upper': top3_ci_upper})\n",
    "\n",
    "# split up the top3 probabilities\n",
    "top1_prob, top2_prob, top3_prob = torch.split(top3_prob, 1, dim=1)\n",
    "\n",
    "# Convert the tensors to NumPy arrays\n",
    "top1_prob_arr = top1_prob.numpy().flatten()\n",
    "top2_prob_arr = top2_prob.numpy().flatten()\n",
    "top3_prob_arr = top3_prob.numpy().flatten()\n",
    "\n",
    "# split up the top3 labels to match \n",
    "top1_label = [sublist[0] for sublist in top3_label]\n",
    "top2_label = [sublist[1] for sublist in top3_label]\n",
    "top3_label = [sublist[2] for sublist in top3_label]\n",
    "\n",
    "# create dataframe of predictions\n",
    "df_pred = pd.DataFrame({\n",
    "    'architecture' : \"EfficientNet-V2-M\",\n",
    "    'random_state' : random_state,\n",
    "    'augmentation' : augmentation,\n",
    "    'gen_folder' : generation_folder,\n",
    "    'generation_type' : generation_type,\n",
    "    'n_training_per_label' : n_per_label,\n",
    "    'n_synthetic_per_real' : n_synthetic_per_real,\n",
    "    'include_synthetic' : include_synthetic,\n",
    "    'md5hash': md5hashes,\n",
    "    'true_label': true_labels,\n",
    "    'top1_label': top1_label,\n",
    "    'top1_prob': top1_prob_arr,\n",
    "    'top2_label': top2_label,\n",
    "    'top2_prob': top2_prob_arr,\n",
    "    'top3_label': top3_label\n",
    "})\n",
    "\n",
    "# log the test predictions\n",
    "wandb.log({\"test_predictions\": wandb.Table(dataframe=df_pred)})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model with 32 real training images per disease condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to produce a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model with 32 real training images per disease condition, plus an additional 10 synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data to compare the performance of these two trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire experiment can be run using this script, although this will take a while to run\n",
    "!python skin_classification_with_augmentation.py \\    \n",
    "    --dataset hugginface_repo \\ \n",
    "    --n_real_per_label_list \"[1, 8, 16, 32, 64, 128, 228]\" \\\n",
    "    --max_batch_size 32 \\\n",
    "    --arg2 value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the data from the complete experiment\n",
    "# We can provide the user code to reproduce this, but save the trial data so that user can still plot our results if they are unable to run the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
