{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import pdb\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Iterable, List, Optional, Union, Literal\n",
    "import pandas as pd\n",
    "import diffusers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionInpaintPipeline, StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torch import Tensor, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TVF\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.rich import tqdm, trange\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and autograd\n",
    "ctx = torch.inference_mode()\n",
    "ctx.__enter__()\n",
    "device = 'cuda'\n",
    "dtype = torch.float16\n",
    "\n",
    "# Set up the experiment\n",
    "prompt = 'An image of {}, a skin disease'\n",
    "resolution = 512\n",
    "batch_size = 16\n",
    "model_type = \"text-to-image\"\n",
    "#pretrained_model_name_or_path = \"runwayml/stable-diffusion-inpainting\"\n",
    "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "start_index = 0\n",
    "num_generations_per_image = 1\n",
    "seed = 42\n",
    "guidance_scale = 3.0\n",
    "num_inference_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173d8f3ff750469e9e2380251919c913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pipeline with 865_910_724 unet parameters\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('Loading model')\n",
    "if model_type == 'inpaint':\n",
    "    pipeline = StableDiffusionInpaintPipeline.from_pretrained(pretrained_model_name_or_path, torch_dtype=dtype,\n",
    "        safety_checker=None, feature_extractor=None, requires_safety_checker=False)\n",
    "elif model_type == 'text-to-image':\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path, torch_dtype=dtype,\n",
    "        safety_checker=None, feature_extractor=None, requires_safety_checker=False)\n",
    "else:\n",
    "    raise ValueError(model_type)\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "pipeline.to(device)\n",
    "\n",
    "print(f'Loaded pipeline with {sum(p.numel() for p in pipeline.unet.parameters()):_} unet parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, instance_prompt, transform=None):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.transform = transform\n",
    "        self.instance_prompt = instance_prompt\n",
    "        \n",
    "        # Build list of image paths and their labels\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Walk through class folders\n",
    "        for class_name in os.listdir(self.dataset_dir):\n",
    "            class_dir = self.dataset_dir / class_name\n",
    "            if not class_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            # Get all jpg images in this class folder\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith('.png'):\n",
    "                    self.image_paths.append(class_dir / img_name)\n",
    "                    self.labels.append(class_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and convert image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_name = image_path.stem\n",
    "        \n",
    "        prompt = self.instance_prompt.format(label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"prompt\": prompt, \"image_name\": image_name, \"pixel_values\": image, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((resolution, resolution)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # Normalize images to [-0.5, 0.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_directory = \"sample_dataset\"\n",
    "\n",
    "custom_dataset = CustomDataset(dataset_directory, instance_prompt = prompt, transform=transform)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1a2d445050>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomness\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed + start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse args\n",
    "# output_dir_path = Path(\"/n/scratch/users/t/thb286/generation_test\")\n",
    "\n",
    "# def get_output_paths(batch: dict, stage: str, idx: int) -> list[Path]:\n",
    "#     return [\n",
    "#         output_dir_path / stage / f'{idx:02d}' / f'{image_name}.png'\n",
    "#         for image_name in batch['image_name']\n",
    "#     ]\n",
    "\n",
    "def save(image, path):\n",
    "    path = Path(path) if isinstance(path, str) else path\n",
    "    path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    image.save(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef817f2304440c7a2524b902dc8c543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26811/3004595500.py:6: TqdmExperimentalWarning: rich is experimental/alpha\n",
      "  for batch_idx, batch in enumerate(tqdm(dataloader)):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[Repeat 0, batch 0] Saved image grid to /n/scratch/users/t/thb286/generation_test/grid/00-batch-00.png\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[Repeat 0, batch 0] Saved image grid to /n/scratch/users/t/thb286/generation_test/grid/00-batch-00.png\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the text-to-image \n",
    "output_dir_path = Path(\"/n/scratch/users/t/thb286/generation_test\")\n",
    "\n",
    "# text-to-image generation\n",
    "for idx in range(start_index, start_index + num_generations_per_image):\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        # Shared arguments\n",
    "        gen_kwargs = dict(\n",
    "            prompt=batch[\"prompt\"],\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            height=resolution,\n",
    "            width=resolution,\n",
    "        )\n",
    "\n",
    "        # Text-to-image\n",
    "        if model_type == 'text-to-image':\n",
    "            #output_paths = get_output_paths(batch, 'text-to-image', idx)\n",
    "\n",
    "            output_paths = [\n",
    "                output_dir_path / \"text-to-image\" / f\"{idx:02d}\" / label / f\"{name}.png\"\n",
    "                for label, name in zip(batch[\"label\"], batch[\"image_name\"])\n",
    "            ]\n",
    "\n",
    "            if all(output_path.is_file() for output_path in output_paths):\n",
    "                continue  # Images have already been generated, skip this batch\n",
    "\n",
    "            # Generate images\n",
    "            images = pipeline(**gen_kwargs).images\n",
    "            assert len(images) == len(output_paths)\n",
    "            for image, path in zip(images, output_paths):\n",
    "                save(image, path)\n",
    "\n",
    "            # Image grid\n",
    "            if batch_idx < 10:\n",
    "                grid_images = [transforms.ToTensor()(img) for img in images]\n",
    "                original_images = [img * 0.5 + 0.5 for img in batch[\"pixel_values\"]]\n",
    "                grid = make_grid(grid_images + original_images, nrow=batch_size, padding=4, pad_value=1.0)\n",
    "                grid_path = output_dir_path / \"grid\" / f'{idx:02d}-batch-{batch_idx:02d}.png'\n",
    "                save(transforms.ToPILImage()(grid), grid_path)\n",
    "\n",
    "                if batch_idx % 1000 == 0:\n",
    "                    print(f'[Repeat {idx}, batch {batch_idx}] Saved image grid to {grid_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "image_path = \"/n/scratch/users/t/thb286/generation_test/00/allergic-contact-dermatitis/0005.png\"\n",
    "#display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "image_path = \"sample_dataset/allergic-contact-dermatitis/0001.png\"\n",
    "#display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single batch of synthetic images, and display in this notebook using the Grid format that Luke designed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following script to generate lots of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, generate about 100 synthetic images from a dummy dataset (with labels) to demonstrate how it works. There should be streamlined functionality to do this (just choose method, backbone, etc.) \n",
    "\n",
    "# generate_synthetic_dataset(\n",
    "# real_images, # needs to be a metadata dataframe -- containing where to find an image, the label, etc.\n",
    "# map_real_to_synthetic_label,  (this would be the hash for fitz, or some kind of unique ID which we need to store)\n",
    "# method = \"text-to-image\", \"inpaint\", \"outpaint\",\n",
    "# text_label, (for text-to-image, this is the image description \"label\")\n",
    "# text_prompt,   # the prompt that will be applied to the label -- this is optional\n",
    "# num_synthetic_per_real,  (defaults to 10)\n",
    "# num_total, # option for specifying the total dataset size we want -- we will handle dataset balance\n",
    "# num_total_type, # options are balanced, same --  works with num_total, do we want to make a balanced dataset or keep the same proportions\n",
    "# output_dir,\n",
    "# model_path = \"xxx\" can set this if we want to use a custom model\n",
    "# )\n",
    "\n",
    "# Then, we can show how to run the script for generating larger amounts of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate_synthetic_dataset(\n",
    "# real_images, \n",
    "# map_real_to_synthetic_label,  (this would be the hash for fitz, or some kind of unique ID which we need to store)\n",
    "# method = \"text-to-image\", \"inpaint\", \"outpaint\",\n",
    "# text_label, (for text-to-image, this is the image description \"label\")\n",
    "# text_prompt,   # the prompt that will be applied to the label -- this is optional\n",
    "# num_synthetic_per_real,  (defaults to 10)\n",
    "# num_total, # option for specifying the total dataset size we want -- we will handle dataset balance\n",
    "# num_total_type, # options are balanced, same --  works with num_total, do we want to make a balanced dataset or keep the same proportions\n",
    "# output_dir,\n",
    "# model_path = \"xxx\" can set this if we want to use a custom model\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python generate.py --output_root generations-pretrained --instance_data_dir=${FITZPATRICK17K_DATASET_DIR} --model_type \"text-to-image\" --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1-base\" --instance_prompt=\"An image of {}, a skin disease\" --disease_class=allergic-contact-dermatitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_synthetic_dataset(\n",
    "#     real_images,\n",
    "#     map_real_to_synthetic_label,\n",
    "#     method=\"text-to-image\",\n",
    "#     text_label=None,\n",
    "#     text_prompt=None,\n",
    "#     num_synthetic_per_real=10,\n",
    "#     num_total=None,\n",
    "#     num_total_type='balanced',\n",
    "#     output_dir=None,\n",
    "#     model_path=\"xxx\"\n",
    "# ):\n",
    "#     for image, label in real_images:\n",
    "#         # Use image and label to generate synthetic images\n",
    "#         pass\n",
    "\n",
    "\n",
    "# real_images parameter should ideally be a PyTorch Dataset object that returns tuples of (image, label). This means that function is flexible and compatible with PyTorch's data handling utilities, making it easier to integrate into training pipelines. we should also use this format for the train/test split functionality we are including\n",
    "\n",
    "\n",
    "# In the function, we should be able to generate an augmented dataset that contains the mask, etc\n",
    "\n",
    "# The longer version of this will run in a .py script with command line args\n",
    "\n",
    "# TODO: make sure the train/test split function uses the same types of inputs, we need to have consistency across functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
