{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from itertools import product\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image directory and fastai path\n",
    "image_dir = \"/n/data1/hms/dbmi/manrai/derm\"\n",
    "path = Path(image_dir)\n",
    "\n",
    "wandb_project = \"dose_response_nov8_2024\"\n",
    "\n",
    "# Set the random seed\n",
    "random_state = 119108\n",
    "\n",
    "# \"generations-more\" appears to contain the synthetic fitzpatrick17k data that was generated for the 9 labels\n",
    "\n",
    "# Set the generation folder\n",
    "generation_folder = \"generations-more/text-to-image/\"\n",
    "#generation_folder = \"generations_test_tb/text-to-image/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the experiment\n",
    "# n_per_label_list = [128, 228]\n",
    "# include_synthetic_list = [True]\n",
    "# n_synthetic_per_real_list = [10, 25, 50, 75]\n",
    "# generation_type_list = ['text-to-image']\n",
    "\n",
    "\n",
    "\n",
    "n_per_label_list = [128]\n",
    "include_synthetic_list = [True]\n",
    "n_synthetic_per_real_list = [10]\n",
    "generation_type_list = ['text-to-image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the different types of augmentation\n",
    "augmentation_dict = {\n",
    "    \"None\": [],\n",
    "    \"aug_transforms\": aug_transforms()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes in a dataframe and returns a dataframe that consists of just n_synthetic_per_real copies of each line in the dataframe\n",
    "# The function also adds a column called location which is the location of the image and a column called synthetic which is True\n",
    "def generate_synthetic_metadata(train_val_df, generation_type, n_synthetic_per_real=10, folder=generation_folder):\n",
    "    df = pd.concat([train_val_df]*n_synthetic_per_real, ignore_index=True)\n",
    "    # create a variable that represents the nth copy of the image\n",
    "    df['n'] = df.groupby('md5hash').cumcount() + 10\n",
    "    df['location'] = folder + df['label'].str.replace(' ', '-')  + '/' + generation_type + '/' + df['n'].astype(str) + '/' + df['md5hash'] + '.png'\n",
    "    df['synthetic'] = True\n",
    "    df['Qc'] = ''\n",
    "    # drop the 'n' column\n",
    "    df = df.drop(columns=['n'])\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in the train data and 2 arguments: n_per_label and include_synthetic.\n",
    "# It selects n_per_label real images from each label and returns a dataframe with the selected images, concatenated with the val data\n",
    "# If include_synthetic is True, it also includes the synthetic images that share the same md5hash as the real images selected\n",
    "def get_n_per_label_data(train_data, val_data, n_per_label, random_state, include_synthetic=False):\n",
    "    # Select n_per_label real images from each label\n",
    "    real_data = train_data[train_data['synthetic'] == False]\n",
    "    n_per_label_data = pd.DataFrame(real_data.groupby(['label']).apply(lambda x: x.sample(n=n_per_label, random_state=random_state)).reset_index(drop=True))\n",
    "    if include_synthetic:\n",
    "        # Get the md5hashes of the real images selected\n",
    "        md5hashes = n_per_label_data['md5hash']\n",
    "        # Get the synthetic images that share the same md5hash as the real images selected\n",
    "        n_per_label_data = train_data[train_data['md5hash'].isin(md5hashes)]\n",
    "    # Concatenate the n_per_label data with the val data\n",
    "    n_per_label_data = pd.concat([n_per_label_data, val_data])\n",
    "    return n_per_label_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the metadata\n",
    "metadata = pd.read_csv(f\"{image_dir}/Fitzpatrick17k/fitzpatrick17k_10label_clean_training.csv\")\n",
    "\n",
    "# # Read in the metadata\n",
    "# metadata = pd.read_csv('../Metadata/fitzpatrick17k_10label_clean_training.csv')\n",
    "\n",
    "# Filter to the top 10 most common labels\n",
    "top_n_labels = metadata['label'].value_counts().index[:9]\n",
    "metadata = metadata[metadata['label'].isin(top_n_labels)].reset_index(drop=True)\n",
    "metadata['location'] = 'Fitzpatrick17k/finalfitz17k/' + metadata['md5hash'] + '.jpg'\n",
    "metadata['synthetic'] = False\n",
    "\n",
    "# Read in the test data\n",
    "test_data = pd.read_csv(f\"{image_dir}/Fitzpatrick17k/fitzpatrick17k_10label_clean_held_out_set.csv\")\n",
    "test_data = test_data[test_data['label'].isin(top_n_labels)].reset_index(drop=True)\n",
    "test_data['location'] = 'Fitzpatrick17k/finalfitz17k/' + test_data['md5hash'] + '.jpg'\n",
    "test_data['synthetic'] = False\n",
    "test_data['is_valid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11394/457346425.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_data = pd.DataFrame(real_train_val_data.groupby(['label']).apply(lambda x: x.sample(n=40, random_state=random_state, replace=False)).reset_index(drop=True))\n",
      "/tmp/ipykernel_11394/4120767880.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  n_per_label_data = pd.DataFrame(real_data.groupby(['label']).apply(lambda x: x.sample(n=n_per_label, random_state=random_state)).reset_index(drop=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real training images per label: 128.0\n",
      "Number of synthetic training images per label: 1280.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27uvhqm8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>error_rate</td><td>▁</td></tr><tr><td>lr_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mom_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>raw_loss</td><td>▇▆█▆█▆▆▄▆▅▄▄▄▅▆▅▅▄▄▄▃▄▂▄▃▃▃▂▂▃▃▁▃▂▃▃▃▂▃▂</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_samples_per_sec</td><td>▁███████████████████████████████████████</td></tr><tr><td>valid_loss</td><td>▁</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wd_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.27222</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>eps_1</td><td>1e-05</td></tr><tr><td>error_rate</td><td>0.72778</td></tr><tr><td>lr_0</td><td>0.001</td></tr><tr><td>lr_1</td><td>0.001</td></tr><tr><td>mom_0</td><td>0.9</td></tr><tr><td>mom_1</td><td>0.9</td></tr><tr><td>raw_loss</td><td>1.82983</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>sqr_mom_1</td><td>0.99</td></tr><tr><td>train_loss</td><td>1.94797</td></tr><tr><td>train_samples_per_sec</td><td>368.89108</td></tr><tr><td>valid_loss</td><td>2.39365</td></tr><tr><td>wd_0</td><td>0.01</td></tr><tr><td>wd_1</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-dream-1</strong> at: <a href='https://wandb.ai/tbu/dose_response_nov8_2024/runs/27uvhqm8' target=\"_blank\">https://wandb.ai/tbu/dose_response_nov8_2024/runs/27uvhqm8</a><br/> View project at: <a href='https://wandb.ai/tbu/dose_response_nov8_2024' target=\"_blank\">https://wandb.ai/tbu/dose_response_nov8_2024</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241108_161854-27uvhqm8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27uvhqm8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thb286/synthetic-derm/wandb/run-20241108_162600-q00vkybx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tbu/dose_response_nov8_2024/runs/q00vkybx' target=\"_blank\">usual-sponge-2</a></strong> to <a href='https://wandb.ai/tbu/dose_response_nov8_2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tbu/dose_response_nov8_2024' target=\"_blank\">https://wandb.ai/tbu/dose_response_nov8_2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tbu/dose_response_nov8_2024/runs/q00vkybx' target=\"_blank\">https://wandb.ai/tbu/dose_response_nov8_2024/runs/q00vkybx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thb286/synthetic-derm/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/thb286/synthetic-derm/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/50 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/99 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the possible values for function parameters\n",
    "for combo in itertools.product(n_per_label_list, include_synthetic_list, n_synthetic_per_real_list, generation_type_list, augmentation_dict):\n",
    "    n_per_label, include_synthetic, n_synthetic_per_real, generation_type, augmentation = combo\n",
    "\n",
    "    # Make sure no train data is in the test set\n",
    "    train_val_data = pd.DataFrame(metadata[~metadata['md5hash'].isin(test_data['md5hash'])])\n",
    "    # Get the synthetic metadata\n",
    "    synthetic_metadata = generate_synthetic_metadata(train_val_data, generation_type=generation_type, n_synthetic_per_real=n_synthetic_per_real, folder = generation_folder)\n",
    "\n",
    "    # Concatenate the synthetic data with the train/val data\n",
    "    train_val_data = pd.concat([train_val_data, synthetic_metadata]).reset_index(drop=True)\n",
    "\n",
    "    # Split the train/val data into train and val, with 40 images from each label in the val set\n",
    "    real_train_val_data = train_val_data[train_val_data['synthetic'] == False]\n",
    "    val_data = pd.DataFrame(real_train_val_data.groupby(['label']).apply(lambda x: x.sample(n=40, random_state=random_state, replace=False)).reset_index(drop=True))\n",
    "    train_data = pd.DataFrame(train_val_data[~train_val_data['md5hash'].isin(val_data['md5hash'])])\n",
    "\n",
    "    # Add an 'is_valid' column to the train and val data\n",
    "    train_data['is_valid'] = False\n",
    "    val_data['is_valid'] = True\n",
    "\n",
    "    # Get the data\n",
    "    df = get_n_per_label_data(train_data, val_data, n_per_label, random_state, include_synthetic)\n",
    "\n",
    "    # Print a summary of the run data\n",
    "    print(\"Number of real training images per label:\", len(df[(df['synthetic']==False) & (df['is_valid']==False)]) /df['label'].nunique())\n",
    "    print(\"Number of synthetic training images per label:\",  len(df[(df['synthetic']==True) & (df['is_valid']==False)]) /df['label'].nunique())\n",
    "\n",
    "    # adjust batch size based on number of images\n",
    "    if (len(df[df.is_valid == False])/10 >= 500):\n",
    "        batch_size = 128\n",
    "    elif (len(df[df.is_valid == False])/10 >= 100):\n",
    "        batch_size = 64\n",
    "    elif (len(df[df.is_valid == False])/10 >= 10):\n",
    "        batch_size = 32\n",
    "    else:\n",
    "        batch_size = 8\n",
    "\n",
    "    # Create a fastai dataloader\n",
    "    dls = ImageDataLoaders.from_df(df, \n",
    "                            path,\n",
    "                            fn_col='location',\n",
    "                            label_col='label',\n",
    "                            valid_col='is_valid', \n",
    "                            bs=batch_size,\n",
    "                            item_tfms=Resize(224),\n",
    "                            batch_tfms=augmentation_dict[augmentation])            \n",
    "\n",
    "    # Set config parameters for wandb\n",
    "    config = dict (\n",
    "        architecture = \"EfficientNet-V2-M\",\n",
    "        gen_folder = generation_folder,\n",
    "        random_state = random_state,\n",
    "        augmentation = augmentation, \n",
    "        n_training_per_label = n_per_label,\n",
    "        include_synthetic = include_synthetic,\n",
    "        n_synthetic_per_real = n_synthetic_per_real,\n",
    "        generation_type = generation_type\n",
    "    )\n",
    "\n",
    "    # set tags for wandb and\n",
    "    sample_tag = \"n_real_per_label_\" + str(n_per_label)\n",
    "    seed_tag = \"seed_\" + str(random_state)\n",
    "    include_synthetic_tag = \"include_synthetic_\" + str(include_synthetic)\n",
    "    generation_type_tag = str(generation_type)\n",
    "\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        tags=[sample_tag, seed_tag, include_synthetic_tag, generation_type_tag],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    learn = vision_learner(dls, \n",
    "                        arch=efficientnet_v2_m,                           \n",
    "                        metrics=[error_rate, accuracy])\n",
    "\n",
    "    # fit with wandb callback\n",
    "    learn.fit(50, cbs=[WandbCallback(), EarlyStoppingCallback (monitor='valid_loss', min_delta=0.0, patience=3),\n",
    "                        SaveModelCallback (monitor='valid_loss', fname='best_model_tb')])\n",
    "\n",
    "    # load the best model\n",
    "    learn.load('best_model_tb')\n",
    "\n",
    "    # get the confusion matrix and log it to wandb\n",
    "    # interp = ClassificationInterpretation.from_learner(learn)\n",
    "    # cm_plot = interp.plot_confusion_matrix(title=f\"N per label:{n_per_label} | Synthetic:{include_synthetic} | Val set\")\n",
    "    # wandb.log({\"Validation Confusion Matrix\": interp.confusion_matrix()})\n",
    "\n",
    "    # predict on test data\n",
    "    test_dl = dls.test_dl(test_data)\n",
    "    # get predictions and probabilities for test set\n",
    "    preds, _ = learn.get_preds(dl=test_dl)\n",
    "    # get predicted labels for top-1 and top-3\n",
    "    top1_pred = torch.argmax(preds, dim=1)\n",
    "    top3_pred = torch.topk(preds, k=3, dim=1).indices\n",
    "\n",
    "    md5hashes = test_data['md5hash']\n",
    "    # get predicted labels and probabilities for top-1 and top-3\n",
    "    top1_prob, top1_label = torch.topk(preds, k=1, dim=1)\n",
    "    top3_prob, top3_label = torch.topk(preds, k=3, dim=1)\n",
    "\n",
    "    # convert tensor labels to class labels\n",
    "    top1_label = [learn.dls.vocab[i] for i in top1_label.squeeze()]\n",
    "    top3_label = [[learn.dls.vocab[j] for j in i] for i in top3_label]\n",
    "\n",
    "    # get true labels for test set\n",
    "    true_labels = test_data['label']\n",
    "\n",
    "    # calculate accuracy scores\n",
    "    top1_acc = (top1_label == true_labels).mean()\n",
    "    top3_acc = torch.zeros(len(true_labels))\n",
    "    for i in range(len(true_labels)):\n",
    "        top3_acc[i] = true_labels[i] in top3_label[i]\n",
    "    top3_acc = top3_acc.mean()\n",
    "\n",
    "    # calculate upper and lower bounds for 95% confidence interval\n",
    "    top1_ci_lower, top1_ci_upper = proportion_confint(top1_acc*len(true_labels), len(true_labels), alpha=0.05, method='normal')\n",
    "    top3_ci_lower, top3_ci_upper = proportion_confint(top3_acc*len(true_labels), len(true_labels), alpha=0.05, method='normal')\n",
    "\n",
    "    # log accuracy scores to wandb\n",
    "    wandb.log({'top1_acc': top1_acc,\n",
    "                'top1_ci_lower': top1_ci_lower,\n",
    "                'top1_ci_upper': top1_ci_upper,\n",
    "                'top3_acc': top3_acc,\n",
    "                'top3_ci_lower': top3_ci_lower,\n",
    "                'top3_ci_upper': top3_ci_upper})\n",
    "\n",
    "    # split up the top3 probabilities\n",
    "    top1_prob, top2_prob, top3_prob = torch.split(top3_prob, 1, dim=1)\n",
    "\n",
    "    # Convert the tensors to NumPy arrays\n",
    "    top1_prob_arr = top1_prob.numpy().flatten()\n",
    "    top2_prob_arr = top2_prob.numpy().flatten()\n",
    "    top3_prob_arr = top3_prob.numpy().flatten()\n",
    "\n",
    "    # split up the top3 labels to match \n",
    "    top1_label = [sublist[0] for sublist in top3_label]\n",
    "    top2_label = [sublist[1] for sublist in top3_label]\n",
    "    top3_label = [sublist[2] for sublist in top3_label]\n",
    "\n",
    "    # create dataframe of predictions\n",
    "    df_pred = pd.DataFrame({\n",
    "        'architecture' : \"EfficientNet-V2-M\",\n",
    "        'random_state' : random_state,\n",
    "        'augmentation' : augmentation,\n",
    "        'gen_folder' : generation_folder,\n",
    "        'generation_type' : generation_type,\n",
    "        'n_training_per_label' : n_per_label,\n",
    "        'n_synthetic_per_real' : n_synthetic_per_real,\n",
    "        'include_synthetic' : include_synthetic,\n",
    "        'md5hash': md5hashes,\n",
    "        'true_label': true_labels,\n",
    "        'top1_label': top1_label,\n",
    "        'top1_prob': top1_prob_arr,\n",
    "        'top2_label': top2_label,\n",
    "        'top2_prob': top2_prob_arr,\n",
    "        'top3_label': top3_label\n",
    "    })\n",
    "\n",
    "    # log the test predictions\n",
    "    wandb.log({\"test_predictions\": wandb.Table(dataframe=df_pred)})\n",
    "\n",
    "    # Finish the run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
